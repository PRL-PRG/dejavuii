---
title: "R Notebook"
output: html_notebook
---

# TODO Things to check

- can we say that csv files have headers if the first line starts with `#`? would greatly simplify the c++ reader
- big cleanup of the various commands we have in main, such as determining some decent order and so on...
- cleanup of the settings options (ideally have them globally and only let the commands decide which ones they want to use so that we do not have millions of commands)


# Preparing the data

> Because the original data from the scala downloader is very limited we have to add extra pieces of data to it. For now, this comes from various places and is a bit of a mess. Should be fixed into something more orderly, but this is my first attempt to document what needs to be done. 

## Joining the chunks together

First, the output of the scala downloader chunks should be joined into a single files for commits, projects, paths and file records. During this phase, ids for projects, commits, paths and snapshots are generated and used throughout the rest of the analysis to conserve space. 

`scala-join` command

## Downloading Commit histories

The command `download-commit-histories` downloads all commit histories for the existing projects. Currently takes about 8 days to proceed. The output is two files, one (`commit-history-index.csv`) containing the index in the form of:

    project id, offset in the second file
    
The second file (`commit-history.txt`) contains for each project list of all commits and their parents in the following form: Each project record starts with line `# pid` for debugging purposes, followed by a line for each commit with the following info:

    commit hash, author time, commit time, parent 1 hash, ... parent n hash -- tag (if any)

> Currently these live in `/array/dejavu/data/processed`

## Augmenting Projects

For each project, we want to have extra information available, notably the time the project was created, and number of authors, committers and watchers. For now, we get the information from GHTorrent. So download GHTorrent and then extract only the files we need to save space:

    wget TODO TODO TODO I CAN'T SEE THE DOWNLOADS PAGE ON GHTORRENT WORKING ATM
    tar --extract --file=mysql-2019-01-01.tar.gz mysql-2019-01-01/projects.csv
    tar --extract --file=mysql-2019-01-01.tar.gz mysql-2019-01-01/commits.csv
    tar --extract --file=mysql-2019-01-01.tar.gz mysql-2019-01-01/watchers.csv

Then run the `project-extras` command. With default arguments, this creates the augmented projects table of the following schema:

| Column | Description |
| ------ | ----------- |
| 0      | Project id |
| 1      | User name |
| 2      | Repo name |
| 3      | Fork id (-1 for not a fork, -2 for fork of unknown project) |
| 4      | Created at (UNIX timestamp) |
| 5      | # of Committers |
| 6      | # of Authors |
| 7      | # of Watchers |

> The data from GHTorrent are not the best, TBH. Should be replaced with data obtained via the Github API (approx 10 days) 

Here's the output of what I have ran (with my comments what respective numbers mean)

    peta@prl1e:~/devel/dejavuii/build$ ./dejavu project-extras
    OH HAI!
    CAN I HAZ STUFFZ?
    I CAN HAZ project-extras NAO:
    Importing from file /data/dejavuii/data/processed/projects.csv
    Total number of projects 2405680
    Creating projects map based on project names...
    Analyzing GH torrent projects...
    Total projects:    110719632 # total number of GHTorrent projects
    Language projects: 11632118  # JS projects in GhTorrent
    Missing projects:  9291783   # projects on GhT that are not in our database
    Missing forks:     7760942   # how many of the above are forks (our dataset should not contain forks)
    Missing deleted:   302869    # how many of them are deleted (our dataset should not contain deleted projects)
    Projects:          2405680   # total number of our projects
    Unaccounted:       121075    # our projects that are not in GhTorrent -- interesting
    Forks:             26        # our projects which are forks of something that is not in our database
    Missing forks:     6         # projects reported as forks on GhTorrent that are in our dataset - should be verified !! 
    Commits analyzed:  66130615  # number of commits from GH torrent relevant to our projects, not important
    Commits skipped:   209953    # number of commits in ghtorrent for which no project id was available, why??? forks? 
    Affected projects: 2067435   # number of projects from our corpus for which the committers and authors numbers were calculated
    Rows used:         23602206  # not important, number of rows in watchers.csv used
    Affected projects: 600568    # number of projects that have watchers - this is interestingly high
    KTHXBYE!

    real    62m4.128s :(
    user    22m3.283s
    sys     3m18.078s
    
```{r}
projects = read.csv("/data/dejavuii/data/filtered/projects.csv")
```

```{r}
summary(projects$committers)
summary(projects$authors)
summary(projects$watchers)
hist(log10(projects$committers), breaks = 100)
hist(log10(projects$authors), breaks = 100)
```

```{r}

```

## Detecting commit originals

First we must augment each commit with its original project. If commit appears in a single project alone, that project is its original, for commits appearing in multiple projects, the oldest such project is the original. To do this, run the `commit-originals`. It is a bit trickier since it requires that projects have already been updated with the `project-extras` step, but takes the original `commits.csv` and `files.csv`: 

    peta@prl1e:~/devel/dejavuii/build$ time ./dejavu commit-originals
    OH HAI!
    CAN I HAZ STUFFZ?
    I CAN HAZ commit-originals NAO:
    Importing from file /data/dejavuii/data/filtered/projects.csv
    Total number of projects 2405680
    Importing from file /data/dejavuii/data/processed/commits.csv
    Total number of commits 42383751
    Analyzed rows: 1796203304
    Analyzed project-commit pairs: 62518504
    KTHXBYE!
    
    real    17m47.191s
    user    15m0.738s
    sys     2m46.316s

```{r}
commits = read.csv("/data/dejavuii/data/filtered/commits.csv")
```

```{r}
summary(commits)
```


Whoa... There is a commit that is in almost 4k projects:) - and we *do not* have explicit forks in the dataset. 

```{r}
length(commits$time[commits$time == 0])
```

Also we have 135 commits with time being equal to 0. This is slightly annouing, unless the commits really were from 1970:)

> TODO check this!!!!

```{r}
hist(log10(commits$numProjects), breaks = 100)
```
OK, this is as expected... 

## Detecting file hash originals

For this, we use the `snapshot-originals` command. This takes the commits from previous step and snapshots and then determines for each snapshot things like its original commit, number of occurences and so on. 

    OH HAI!
    CAN I HAZ STUFFZ?
    I CAN HAZ snapshot-originals NAO:
    Importing from file /data/dejavuii/data/processed/fileHashes.csv
    Total number of snapshots 99300992
    Importing from file /data/dejavuii/data/filtered/commits.csv
    Total number of commits 42383751
    Rows:                   1796203304
    Identical commit times: 1800262
    Snapshots analyzed:     99300991
    Unique snapshots:   67600837
    Original snapshots: 31700154
    Copies snapshots:   1140860388
    KTHXBYE!
    
    real    77m11.128s
    user    72m11.588s
    sys     4m59.250s

> For now, it takes a lot of time, but that is kind of expected since it walks over all file records. 



```{r}
snapshots = read.csv("/data/dejavuii/data/filtered/snapshots.csv")
```

```{r}
summary(snapshots$occurences)
```

## Determining interesting projects

An interresting project is a project that has at least one of the following:

- has at least one watcher
- has created at least one original snapshot
- has created at least one unique snapshot
- has at least one original commit (i.e. commit somewhere else as well, i.e. has implicit or explicit forks)
- has a sequence of at least 20 consecutive commits for a period of over 3 months where no two adjacent commits are separated by more than 30 days

> TODO perhaps also any project that has been forked at least once? 

The criteria above should provide us with an overapproximation, i.e. some of the selected projects are likely to be pretty boring, but we should not skip any interesting project. 

    peta@prl1e:~/devel/dejavuii/build$ time ./dejavu interesting-projects
    OH HAI!
    CAN I HAZ STUFFZ?
    I CAN HAZ interesting-projects NAO:
    Importing from file /data/dejavuii/data/filtered/commits.csv
    Total number of commits 42383751
    Importing from file /data/dejavuii/data/filtered/projects.csv
    Total number of projects 2405680
    Importing from file /data/dejavuii/data/filtered/snapshots.csv
    Total number of snapshots 99300991
    Summarizing project information...
    Summarizing commit information...
    Summarizing snapshot information...
    Analyzing file records...
    Analyzed 1796203304 file records
    Calculating spans...
     Writing results...
    KTHXBYE!
    
    real    38m18.928s
    user    35m11.200s
    sys     3m7.548s

```{r}
projects = read.csv("/data/dejavuii/data/filtered/projects-interesting.csv")
```

Now that we have the data, let's look at some definitions of interesting projects. 

```{r}
length(projects$watchers[projects$watchers > 0])
```
Ok, actually quite a lot of projects have at least one watcher, though this comes from GhTorrent so take it with a grain of salt. 

Let's add some columns to projects so that we know the project duration and span duration:

```{r}
# in days
projects$duration = (projects$lastCommit - projects$firstCommit) / 3600 / 24
projects$spanDuration = (projects$spanEnd - projects$spanStart) / 3600 / 24
```

And get summaries for some overview:

```{r}
summary(projects)
```

OK, this looks reasonable to be on the first glance. The duration gets super super big a few times, but that is most likely because of a few commits having time of 0 in the original dataset. The median and 3rd quartile look decent. 

```{r}
hist(log10(projects$duration), breaks = 100)
```
Yeah... Note its in days...

Now we can try different metrics for interesting projects. From Jan's email yesterday - project duration over 365 days and # of commits greater than 20:

```{r}
sum(projects$duration > 365 & projects$commits >= 20)
```

Let's see if we add projects with any original content to this lot:

```{r}
sum((projects$duration > 365 & projects$commits >= 20) | projects$originalCommits > 0 | projects$originalSnapshots > 0)
```

Hm... this is much more projects than I would have guessed. Let's add to it also projects that contain unique contents: 

```{r}
sum((projects$duration > 365 & projects$commits >= 20) | projects$originalCommits > 0 | projects$originalSnapshots > 0 | projects$uniqueCommits > 0 | projects$uniqueSnapshots > 0)
```

That's almost all projects...

Different way: How many projects we need if we want to see all original commits:

```{r}
sum(projects$originalCommits > 0)
```

If we add files to it, it grows back to the 900k projects. This is expected - commits should be only rarely shared:

```{r}
sum(projects$originalCommits > 0 | projects$originalSnapshots > 0)
```

```{r}
library(hexbin)
plot(hexbin(x = projects$originalSnapshots / projects$snapshots, y = (projects$originalCommits + projects$uniqueCommits) / projects$commits))
```


